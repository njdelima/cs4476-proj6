function net = proj6_ec_part1_cnn_init()
%code for Computer Vision, Georgia Tech by James Hays
%based of the MNIST example from MatConvNet

rng('default');
rng(0);

% constant scalar for the random initial network weights. You shouldn't
% need to modify this.
f=1/100; 

net.layers = {} ;

% 1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(7,7,1,64, 'single'), zeros(1, 64, 'single')}}, ...
                           'stride', 2, ...
                           'pad', 0, ...
                           'name', 'conv1') ;
% 2
net.layers{end+1} = struct('type', 'relu') ;

% 3
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', 0) ;
% 4
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(5,5,64,128, 'single'), zeros(1, 128, 'single')}}, ...
                           'stride', 2, ...
                           'pad', 2, ...
                           'name', 'conv2') ;
% 5
net.layers{end+1} = struct('type', 'relu') ;

% 6
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', 0) ;
% 7
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(3,3,128,256, 'single'), zeros(1, 256, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 1, ...
                           'name', 'conv3') ;
% 8
net.layers{end+1} = struct('type', 'relu') ;

% 9
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(3,3,256,512, 'single'), zeros(1, 512, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'conv4') ;
% 10
net.layers{end+1} = struct('type', 'relu') ;

% 11
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', 0) ;
                       
% 12
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(5,5,512,4096, 'single'), zeros(1, 4096, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'conv5') ;
% 13
net.layers{end+1} = struct('type', 'relu') ;

% 14
net.layers{end+1} = struct('type', 'dropout', ...
                           'rate', 0.5) ;

% 15
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(1,1,4096,250, 'single'), zeros(1, 250, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'fc1') ;
                      
% Loss layer
net.layers{end+1} = struct('type', 'softmaxloss') ;

net = vl_simplenn_tidy(net);

% %You can insert batch normalization layers here
% net = insertBnorm(net, 1) ;
% net = insertBnorm(net, 5) ;
% net = insertBnorm(net, 10) ;

% Visualize the network
vl_simplenn_display(net, 'inputSize', [224 224 1 50])


% --------------------------------------------------------------------
function net = insertBnorm(net, layer_index)
% --------------------------------------------------------------------
assert(isfield(net.layers{layer_index}, 'weights'));
ndim = size(net.layers{layer_index}.weights{1}, 4);
layer = struct('type', 'bnorm', ...
               'weights', {{ones(ndim, 1, 'single'), zeros(ndim, 1, 'single')}}, ...
               'learningRate', [1 1 0.05]) ;
net.layers{layer_index}.weights{2} = [] ;  % eliminate bias in previous conv layer
net.layers = horzcat(net.layers(1:layer_index), layer, net.layers(layer_index+1:end)) ;



